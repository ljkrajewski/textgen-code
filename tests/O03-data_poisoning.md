# Training Data Poisoning
## Description
Training data poisoning is a type of adversarial attack where malicious data is inserted into the training set with the intent to subvert the model's behavior. I'll illustrate a simple example using a text classification model, where the goal of the attacker is to make the model misclassify certain inputs by poisoning the training data.
## Examples
- **Steps**
  - 
## Mitigations
